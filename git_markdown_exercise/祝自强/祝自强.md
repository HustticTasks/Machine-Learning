# Linear Regression

为了执行有监督学习，我们必须先定义出一个函数，用来预测 y 。这里我们采用一个关于输入 X 的线性函数。

$$h_\theta(x)=\theta_0+\theta_1x_1+\theta_2x_2$$

在这里，theta就是需要学习的参数（也可以称之为权重）。一般，为了简化，我们让 X0=1 ，于是

$$h_\theta(x)=\sum^n_{i=0}\theta_ix_i=\theta^Tx$$

已经有了训练集，怎么选择或者学习参数θ？
假设 h(x) 和 真实值 y 越接近，说明参数越好。为了判断二者的接近程度，我们定义误差函数，用来有目的性的学习参数：

$$J(\theta)=\frac12\sum^m_{i=1}(h_\theta(x^{(i)})-y^{(i)})^2$$

这个目标函数，也就是普通最小二乘回归模型。

# 1.LMS（least mean squares）algorithm

为了使损失函数达到最小值，我们采用梯度下降法，对每个θ_i求偏导，并更新θ_i。

![最小二乘](5945886-d929ddb44518faff.png)

*补充一点，从上图中可以看出，最小二乘法度量的是，样本点到直线的坐标轴距离 d1，不是样本点到直线的距离 d2*

## **-- 对于梯度下降法的补充**

##### 1.batch gradient decent

$$\theta_j:=\theta_j+\alpha\sum^m_{i=1}(y^{(i)}-h_\theta(x^{(i)}))x_j^{(i)}\text{       (for every j)}$$

从中可以看出，Σ后面得到内容，就是损失函数J对θ_j求的偏导（因为J也是所有的m个样例预测值和真实值的**误差之和**）。

那么，根据更新法则，对于第 j 个参数，**需要计算出从1到m个样例误差**再乘以样例 元素X_j并求和。从几何意义上来说，这个时候求得的梯度，是最优的梯度。**但是计算量比较大。**

##### 2.stochastic gradient decent

$$\theta_j:=\theta_j+\alpha(y^{(i)}-h_\theta(x^{(i)}))x^{(i)}_j \text{    (for every j)}$$



从中可以看出，随机梯度下降法，对于第 j 个参数，**只需要计算第 i 个样例的误差即可更新**，也就是，只要来一个样例，就可以更新参数θ。虽然不是最优的梯度，但效果挺好。

##### **3.梯度下降法 和 梯度上升法 的解释**

![5945886-254e1c3e7928f429](5945886-254e1c3e7928f429.png)

如图，可以看到，当自变量X随着导数的方向变化时，函数会增大。

梯度就是对各个自变量求偏导，定义为**由偏导数组成的梯度是指向函数值增大的方向**，再结合上面单个 自变量，引申到多个自变量可知，一个坐标系中的**一个点沿着梯度的方向变化时，会使函数沿着变化最大的方向增大。**

所以可以知道，梯度下降，就是 X 减去梯度（即加上梯度的反方向），使函数值变小，不断更新达到最小值。  ---  梯度上升，就是X加上梯度，使函数值不断变大。

以上就是对梯度下降法的理解。

# 2.正规方程（The normal equations）

除了梯度下降法，可以用另一种方法来最小化J，并且可以直接求出。具体推导过程就不赘述了，直接看结果：

$$\theta=(X^TX)^{-1}{x^T}\overrightarrow{y}$$

# 3. Probabilistic interpretation （用概率来解释最小二乘法的合理性）

*也可以理解为，为什么用“二次”的误差和来衡量预测值逼近真实值的程度，而不是绝对值或者四次方。*

**根据极大似然的思想，我们可以认为，要想使预测值 Y 向量最逼近真实值，就是想要找到极大似然函数的估计值 θ**

### 1.怎么样让预测值最接近真实值呢？ 采用极大似然。

首先，我们假设想要预测的目标值和输入之间符合方程：

$$y^{(i)}=\theta^Tx^{(i)}+\epsilon^{(i)}$$

这里，ε 代表误差项（包括了模型误差还有噪音等） 。我们假设 ε 符合独立同分布的 均值为零，方差是 σ² 的高斯分布。那么 ε 的概率密度可以表示为：

$$p(\epsilon^{(i)})=\frac1{\sqrt{2\pi}\sigma}exp(-\frac{(\epsilon^{(i)})^2}{2\sigma^2})$$

### 2.怎么把输入和输出也放入极大似然的方程呢？ 假设误差独立同分布，用二者之差代替误差。

这也意味着：

$$p(y^{(i)}|x^{(i)};\theta)=\frac1{\sqrt{2\pi}sigma}exp(-\frac{(y^{(i)}-\theta^Tx^{(i)})^2}{2\sigma^2})$$

上面的概率密度意思为 在参数θ下（这里不是限制θ,可以当θ不存在），给定输入x，输出预测值y的概率密度。

同理，可以把整个数据集X作为输入，整个预测向量Y作为输出，于是 可以得到极大似然函数：

$$L(\theta)=L(\theta;X,\overrightarrow{y})=p(\overrightarrow{y}|X;\theta)$$

我们想要做的，就是找到一个最好的θ，使得极大自然函数值最大，也就是，这个θ能够预测出最准确的y值。

基于ε符合独立同分布的假设，极大似然函数可以写成以下形式：

$$L(\theta)=\prod^m_{i=1}p(y^{(i)}|x^{(i)};\theta)=\prod^m_{i=1}\frac1{\sqrt{2\pi}\sigma}exp(-\frac{(y^{(i)}-\theta^Tx^{(i)})^2}{2\sigma^2})$$

### 3.用log思想，极大化似然函数值，于是得到极小化最小二乘的结论。

此时，能够使极大似然函数值最大的 θ ，也必然能够使log L(θ)最大化，即：

$$l(\theta)=logL(\theta)=log\prod^m_{i=1}\frac1{\sqrt{2\pi}\sigma}exp(-\frac{(y^{(i)}-\theta^Tx^{(i)})^2}{2\sigma^2})=\sum^m_{i=1}log\frac1{\sqrt{2\pi}\sigma}-\frac1{\sigma^2}\cdot\frac12\sum^m_{i=1}(y^{(i)}-\theta^Tx^{(i)})^2$$

于是，最大化 log L(θ) 等同于 极小化

$$\frac12\sum^m_{i=1}(y^{(i)}-\theta^Tx^{(i)})^2$$

也就是，最小二乘误差函数J(θ）。

也就是说，最小二乘回归的解，也就是 θ 的极大似然估计，能够使得模型预测出最接近真实值的解。

到这里，说明了最小二乘回归模型的解是合理的。**其思想，是先表示出 预测值和真实值的方程，让误差符合独立同分布（这是重要的假设），利用极大似然的思想，将极大似然函数估计和最小二乘回归联系起来**

# 4. Locally weighted linear regression

先用一张图片，简单说明一下过拟合和欠拟合的问题![5945886-79219ee7b5f89b88](5945886-79219ee7b5f89b88.png)

从图中可以看出，**对于同一个数据集**
最左边只采用一个特征x，得到的预测图形是一条直线，预测不准确，即欠拟合。
中间多加了一个特征x²，对于数据的拟合好的多

看起来，加的特征越多，对于这个数据拟合的就越好，但是就像最右边的 图形，把特征加到五个时，完美的拟合了所有的数据，但如果新出现一个数据，可能误差会很大，这就是过拟合。

从上可以得出两点结论，采用上述回归预测时：

- 1 并不是特征越多越好，容易产生过拟合
- 2 第一条结论的原因，就是，以上模型一旦学习完毕，参数θ就是确定的了，无论对于什么样的输入，输出都是根据当前的θ来预测的。

## **那么，如果有这种数据集怎么办？**

![5945886-7500f7dcc9fc8df5](5945886-7500f7dcc9fc8df5.png)

很明显，此时采用一个特征，两个特征，无论几个特征，都不合适。因为一旦参数θ确定，模型就确定了，那么都没办法很好地拟合。

所以，**根据数据集的特点，设计学习算法是很重要的**。

此时，可以采用，局部加权线性回归，思想是

- 1 只学习当前输入x周围的局部数据集，用来拟合x周围的局部数据

- 2 由于这种只学习局部的特性，每次参数θ都不相同。所以也需要**预测一次，就需要重新学习一次参数θ**

![5945886-42c478c4804de192](5945886-42c478c4804de192.png)

如图所示，输入为X1 的时候，就用直线拟合（个人觉得不一定非要直线，根据数据集的不同，可能二次函数的拟合更合适）X1附近的数据，用来预测。同理输入为X2时，再学习一次。

算法的改变之处在于，**在每一个样例误差前面，加了一个权值**，距离需要预测的x越近的数据集，权值越大，拟合的贡献越大。越远，贡献越小。

这是之前的算法：

$$\text{1. Fit }\theta\text{ to minimize }\sum_i(y^{(i)}-\theta^Tx^{(i)})^2$$

$$\text{2. Output }\theta^Tx$$

这是加了权值之后的算法： 

$$\text{1. Fit }\theta\text{ to minimize }\sum_iw^{(i)}(y^{(i)}-\theta^Tx^{(i)})^2$$

$$\text{2. Output }\theta^Tx$$

其中，权值的计算公式不是唯一的，根据不同的目的，不同的思想，权值的计算公式也是不一样的。此处，采用一个比较标准的计算公式：

$$w^{(i)}=exp(-\frac{(x^{(i)}-x)^2}{2\tau^2})$$

从中可以看出，当样例x（i）越接近我们想要预测的点x，|x_i - x|的值越小，此时权值越接近1。

参数τ控制了随着样例和 x 点距离的增加，权值下降的快慢的程度。（又叫做带宽参数--bandwidth parameter）.τ越小，下降的越快。

# 5. 权重大小的设置问题

机器学习的目标是为了学习到特定的参数，从而根据输入预测结果。

设置权重公式的规律就是：**你认为越有用的信息，就让它的权值越大**。（这里的“有用”是相对的，有时类似的目标函数，权值的大小设置可能相反。）

**权值的设置，与目标函数的min或者max并不冲突，权值的设置，是为了学习到更好的参数θ**

列子：

- 1 局部加权线性回归中，认为距离需要预测的样本越近的样例越有用，则这些样例的权值就越大。
- 2 推荐系统的矩阵因式分解中，可以**认为**（这是看你自己的想法了），当 评分为5的时候，说明用户越重视这个商品，那么这个信息越有用，所以可以将这个样例的权值设置的比较大。



$$\text{min}\omega_ui(r_{ui}-\overrightarrow{x}_u\cdot\overrightarrow{y}_i)^2+\lambda||x||_2+\lambda||y||_2$$

比如，此时目标函数是min，但是，如果评分r等于5（你认为有用）
那么就增大权值。

具体的解释就是，当权值变大时，整体会变大，而目标函数是min，所以相当于惩罚了这个x和y向量，相对于别的x，y向量，会更加迫使这两个x和y向量更加接近评分r，评分大的向量更接近真实值，那么预测时也会更准确。



